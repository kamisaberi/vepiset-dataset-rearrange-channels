{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.199310Z",
     "start_time": "2025-05-09T14:04:02.359199Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import setproctitle\n",
    "import random\n",
    "import copy\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.loss.binary_cross_entropy import BinaryCrossEntropy\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import random\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.254177Z",
     "start_time": "2025-05-09T14:04:04.252527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "setproctitle.setproctitle(\"spike_train\")\n",
    "sys.path.append('.')\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "50c0ef71c6141ad8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.299320Z",
     "start_time": "2025-05-09T14:04:04.298050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numpy_files_path = \"/run/media/kami/SSD/DATASETS/vepiset-dataset/NPY-Files/\"\n",
    "save_csv_file = \"./data.csv\""
   ],
   "id": "77a3edfef6009abb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Configuration",
   "id": "35a4b5e7c71cb5d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.350069Z",
     "start_time": "2025-05-09T14:04:04.347916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)  # cpu\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ],
   "id": "7bc5937304439022",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.401496Z",
     "start_time": "2025-05-09T14:04:04.396538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = edict()\n",
    "\n",
    "config.TRAIN = edict()\n",
    "\n",
    "config.TRAIN.process_num = 1\n",
    "\n",
    "config.TRAIN.batch_size = 128\n",
    "config.TRAIN.validatiojn_batch_size = config.TRAIN.batch_size\n",
    "config.TRAIN.accumulation_batch_size = 128\n",
    "config.TRAIN.log_interval = 10\n",
    "config.TRAIN.test_interval = 1\n",
    "config.TRAIN.epoch = 1\n",
    "\n",
    "config.TRAIN.init_lr = 0.0005\n",
    "config.TRAIN.lr_scheduler = 'cos'\n",
    "\n",
    "if config.TRAIN.lr_scheduler == 'ReduceLROnPlateau':\n",
    "    config.TRAIN.epoch = 1\n",
    "    config.TRAIN.lr_scheduler_factor = 0.1\n",
    "\n",
    "config.TRAIN.weight_decay_factor = 1.e-2\n",
    "config.TRAIN.vis = False\n",
    "\n",
    "config.TRAIN.warmup_step = 1500\n",
    "config.TRAIN.opt = 'Adamw'\n",
    "\n",
    "config.TRAIN.gradient_clip = 5\n",
    "\n",
    "config.TRAIN.vis_mixcut = False\n",
    "if config.TRAIN.vis:\n",
    "    config.TRAIN.mix_precision = False\n",
    "else:\n",
    "    config.TRAIN.mix_precision = False\n",
    "\n",
    "config.MODEL = edict()\n",
    "\n",
    "config.MODEL.model_path = './trained_models/'\n",
    "\n",
    "config.DATA = edict()\n",
    "\n",
    "config.DATA.data_file = save_csv_file\n",
    "\n",
    "config.DATA.data_root_path = 'utils'\n",
    "\n",
    "config.MODEL.early_stop = 30\n",
    "\n",
    "config.MODEL.pretrained_model = None\n",
    "\n",
    "config.SEED = 10086\n",
    "\n",
    "seed_everything(config.SEED)\n",
    "config.is_base = 1\n"
   ],
   "id": "4abea7a354f85044",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Create CSV File From NPY Files",
   "id": "c040671250bbcfdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Variables",
   "id": "8eb2b1a54addd6c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.443813Z",
     "start_time": "2025-05-09T14:04:04.442509Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "640dbe47f11e7b66",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.495662Z",
     "start_time": "2025-05-09T14:04:04.492418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_npy_files(folder_path):\n",
    "    npy_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.npy'):\n",
    "                npy_files.append(os.path.join(root, file))\n",
    "\n",
    "    return npy_files\n",
    "\n",
    "\n",
    "def get_data(data_dir):\n",
    "    samples = find_npy_files(data_dir)\n",
    "    labels = [1 if int(x.split(\"__\")[1].split(\".\")[0]) > 1 else int(x.split(\"__\")[1].split(\".\")[0]) for x in samples]\n",
    "    train_val = [0] * len(samples)\n",
    "    return samples, labels, train_val\n",
    "\n",
    "\n",
    "def get_data_files(numpy_files_path):\n",
    "    #data_dir = npy_val_data\n",
    "    fns_list = []\n",
    "    labels_list = []\n",
    "    train_val_list = []\n",
    "    samples, labels, train_val = get_data(numpy_files_path)\n",
    "    fns_list.extend(samples)\n",
    "    labels_list.extend(labels)\n",
    "    train_val_list.extend(train_val)\n",
    "    # print(\"train_val_list\" , train_val_list)\n",
    "    return fns_list, labels_list, train_val_list\n"
   ],
   "id": "8852bd9026067bf",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.614342Z",
     "start_time": "2025-05-09T14:04:04.542009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_fns_list, val_labels_list, val_vals_list = get_data_files(numpy_files_path)\n",
    "# print(val_fns_list)\n",
    "# print( val_labels_list)\n",
    "# print( val_vals_list)\n",
    "submission = pd.DataFrame({'file_path': val_fns_list,\n",
    "                           'target': val_labels_list,\n",
    "                           'train_val': val_vals_list})\n",
    "# submission\n",
    "### split train - val = 8:2\n",
    "indices = submission[submission['train_val'] == 0].index\n",
    "val_num = len(indices) // 5\n",
    "indices_to_change = np.random.choice(indices, val_num, replace=False)\n",
    "submission.loc[indices_to_change, 'train_val'] = 1\n",
    "\n",
    "# print(\"fns len:\", len(val_fns_list))\n",
    "# print(\"label len:\", len(val_labels_list))\n",
    "# print(\"val len:\", val_num)\n",
    "# print(\"train:val {0}:{1}\".format(8, 2))\n",
    "submission.to_csv(save_csv_file, index=False)\n",
    "# submission\n",
    "# submission.head()\n"
   ],
   "id": "12b614367629b6ba",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Train",
   "id": "f61cc0f557fde3d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Logger",
   "id": "df0216595dcf2fe6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.623646Z",
     "start_time": "2025-05-09T14:04:04.621889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_logger(LEVEL, log_file=None):\n",
    "    head = '[%(asctime)-15s] [%(levelname)s] %(message)s '\n",
    "    if LEVEL == 'info':\n",
    "        logging.basicConfig(level=logging.INFO, format=head)\n",
    "    elif LEVEL == 'debug':\n",
    "        logging.basicConfig(level=logging.DEBUG, format=head)\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    if log_file != None:\n",
    "        fh = logging.FileHandler(log_file)\n",
    "        logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = get_logger('info')\n"
   ],
   "id": "44216f31eafd00b0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Models",
   "id": "1832b346c212745f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### AUG",
   "id": "19734eabb2a0bc85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.672320Z",
     "start_time": "2025-05-09T14:04:04.669089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AUG(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.size(0)\n",
    "        for i in range(bs):\n",
    "            if random.uniform(0, 1) < 0.5:\n",
    "                x[i, ...] = self.pitch_shift_spectrogram(x[i, ...])\n",
    "            if random.uniform(0, 1) < 0.0:\n",
    "                x[i, ...] = self.time_shift_spectrogram(x[i, ...])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def do_cut_out(self, x):\n",
    "\n",
    "        h = 128\n",
    "        w = 128\n",
    "        line_width = random.randint(1, 8)\n",
    "\n",
    "        if random.uniform(0, 1) < 0.5:\n",
    "\n",
    "            start = random.randint(0, w - line_width)\n",
    "            x[:, :, start:start + line_width] = 0\n",
    "        else:\n",
    "            start = random.randint(0, h - line_width)\n",
    "            x[:, start:start + line_width, :] = 0\n",
    "\n",
    "        return x\n",
    "\n",
    "    def pitch_shift_spectrogram(self, spectrogram):\n",
    "        \"\"\" Shift a spectrogram along the frequency axis in the spectral-domain at\n",
    "        random\n",
    "        \"\"\"\n",
    "        nb_cols = spectrogram.size(1)\n",
    "        max_shifts = nb_cols // 50  # around 5% shift\n",
    "        nb_shifts = random.randint(-max_shifts, max_shifts)\n",
    "\n",
    "        return torch.roll(spectrogram, nb_shifts, dims=[1])\n",
    "\n",
    "    def time_shift_spectrogram(self, spectrogram):\n",
    "        \"\"\" Shift a spectrogram along the frequency axis in the spectral-domain at\n",
    "        random\n",
    "        \"\"\"\n",
    "        nb_cols = spectrogram.size(2)\n",
    "        max_shifts = nb_cols // 2  # around 100% shift\n",
    "        nb_shifts = random.randint(-max_shifts, max_shifts)\n",
    "\n",
    "        return torch.roll(spectrogram, nb_shifts, dims=[2])\n",
    "\n",
    "\n"
   ],
   "id": "c7a8e8e5cd9a43c3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Transform",
   "id": "4b79837ac4faab21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.719266Z",
     "start_time": "2025-05-09T14:04:04.716209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transform(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.wave_transform = torchaudio.transforms.Spectrogram(n_fft=256, hop_length=16, power=1, pad_mode='reflect')\n",
    "\n",
    "    def forward(self, x):\n",
    "        image = self.wave_transform(x)\n",
    "        return image\n",
    "\n",
    "\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n"
   ],
   "id": "30812744e381d998",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MLP",
   "id": "bb0e9af72ea41c2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.765627Z",
     "start_time": "2025-05-09T14:04:04.763397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(feature_size, 6)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.linear2 = nn.Linear(6, 24)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.linear3 = nn.Linear(24, 24)\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "id": "832f2156b0785f0f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### NET",
   "id": "773e05b2b6d57e19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.810983Z",
     "start_time": "2025-05-09T14:04:04.808885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=1, add_channel=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.preprocess = Transform()\n",
    "\n",
    "        self.model = timm.create_model('vgg16',\n",
    "                                       pretrained=True,\n",
    "                                       in_chans=19)\n",
    "\n",
    "        self.avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512, num_classes, bias=True)\n",
    "        self.add_sleep_feature = MLP(1)\n",
    "        weight_init(self.fc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # do preprocess\n",
    "        # print(\"FORWARD 1\")\n",
    "        bs = x.size(0)\n",
    "        # print(\"FORWARD 2\")\n",
    "        x = self.preprocess(x)\n",
    "        # print(\"FORWARD 3\")\n",
    "        x = self.model.forward_features(x)\n",
    "        # print(\"FORWARD 4\")\n",
    "        fm = self.avg_pooling(x)\n",
    "        # print(\"FORWARD 5\")\n",
    "        fm = fm.view(bs, -1)\n",
    "        # print(\"FORWARD 6\")\n",
    "        feature = self.dropout(fm)\n",
    "        # print(\"FORWARD 7\")\n",
    "        x = self.fc(feature)\n",
    "        # print(\"FORWARD 8\")\n",
    "\n",
    "        return x\n"
   ],
   "id": "f407276e153b234d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### AlaskaDataIter",
   "id": "643773ec850b0a60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.861834Z",
     "start_time": "2025-05-09T14:04:04.856224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AlaskaDataIter():\n",
    "    def __init__(self, df,\n",
    "                 training_flag=True, shuffle=True):\n",
    "\n",
    "        self.training_flag = training_flag\n",
    "        self.shuffle = shuffle\n",
    "        self.raw_data_set_size = None\n",
    "\n",
    "        self.df = df\n",
    "        logger.info(' contains%d samples  %d pos' % (len(self.df), np.sum(self.df['target'] == 1)))\n",
    "        logger.info(' contains%d samples' % len(self.df))\n",
    "\n",
    "        logger.info(' After filter contains%d samples  %d pos' % (len(self.df), np.sum(self.df['target'] == 1)))\n",
    "        logger.info(' After filter contains%d samples' % len(self.df))\n",
    "\n",
    "        self.leads_nm = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T3', 'T4', 'T5',\n",
    "                         'T6',\n",
    "                         'Fz', 'Cz', 'Pz',\n",
    "                         'PG1', 'PG2', 'A1', 'A2',\n",
    "                         'ECG1', 'ECG2', 'EMG1',\n",
    "                         'EMG2', 'EMG3', 'EMG4']\n",
    "\n",
    "        self.leads_dict = {value: index for index, value in enumerate(self.leads_nm)}\n",
    "\n",
    "        self.left_brain = ['Fp1', 'F3', 'C3', 'P3', 'O1', 'T5', 'T3', 'F7']\n",
    "        self.right_brain = ['Fp2', 'F4', 'C4', 'P4', 'O2', 'T6', 'T4', 'F8']\n",
    "\n",
    "    def filter(self, df):\n",
    "\n",
    "        df = copy.deepcopy(df)\n",
    "        pos_indx = df['target'] == 1\n",
    "        pos_df = df[pos_indx]\n",
    "\n",
    "        neg_indx = df['target'] == 0\n",
    "        neg_df = df[neg_indx]\n",
    "\n",
    "        neg_df = neg_df.sample(frac=1)\n",
    "\n",
    "        dst_df = neg_df\n",
    "        for i in range(1):\n",
    "            dst_df = dst_df._append(pos_df)\n",
    "        dst_df.reset_index()\n",
    "\n",
    "        return dst_df\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        fname = self.df.iloc[item]['file_path']\n",
    "        label = self.df.iloc[item]['target']\n",
    "        try:\n",
    "            fname = fname.strip()\n",
    "            waves = np.load(fname)\n",
    "            # print(\"Init waves (Shape): \" , waves.shape)\n",
    "        except Exception as e:\n",
    "            print(\"=====fname====exception:\", fname, e)\n",
    "            waves = np.zeros(shape=[29, 2000])\n",
    "            label = 0\n",
    "\n",
    "        waves = self.norm(waves)\n",
    "        # print(\"After Norm - waves (Shape): \" , waves.shape)\n",
    "\n",
    "        # Normalize :19 channels\n",
    "        waves = copy.deepcopy(waves)\n",
    "        meadn = np.mean(waves[:19, :], axis=0)\n",
    "        avg_lead = waves[:19, :] - meadn\n",
    "\n",
    "        # print('avg_lead (Shape):', avg_lead.shape)\n",
    "\n",
    "        if self.training_flag and random.uniform(0, 1) < 1.:\n",
    "            waves[:19, :] = self.xshuffle(waves[:19, :])\n",
    "            avg_lead = self.xshuffle(avg_lead)\n",
    "        # print(\"After Shuffle - waves (Shape): \" , waves.shape, \" avg_lead (Shape): \" , avg_lead.shape)\n",
    "        # waves = np.concatenate([waves, avg_lead], axis=0)\n",
    "\n",
    "        label = np.expand_dims(label, -1)\n",
    "\n",
    "        C, L = waves.shape\n",
    "        # print(\"After Concat (Shape): \" , waves.shape, \" avg_lead (Shape): \" , avg_lead.shape)\n",
    "\n",
    "        waves = waves[:19, ...]\n",
    "        if L < 2000:\n",
    "            waves = np.pad(waves, ((0, 0), (0, 2000 - L)), 'constant', constant_values=0)\n",
    "        elif L > 2000:\n",
    "            waves = waves[:, 2000]\n",
    "        waves = np.ascontiguousarray(waves)\n",
    "        # print(\"Final (Shape): \" , waves.shape, \" avg_lead (Shape): \" , avg_lead.shape)\n",
    "\n",
    "        return waves, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def norm(self, wave):\n",
    "\n",
    "        wave[:23, ...] = wave[:23, ...] / 1e-3\n",
    "        wave[23:, ...] = wave[23:, ...] / 1e-2\n",
    "        return wave\n",
    "\n",
    "        # heart_wave = wave[23, :] - wave[24, :]\n",
    "        # muscle_wave1 = wave[25, :] - wave[26, :]\n",
    "        # muscle_wave2 = wave[27, :] - wave[28, :]\n",
    "        # heart_muscle = np.stack([heart_wave, muscle_wave1, muscle_wave2], axis=0)\n",
    "        # wave_26 = np.concatenate([wave[:23, ...], heart_muscle], axis=0)\n",
    "        # return wave_26\n",
    "\n",
    "    def xshuffle(self, wave):\n",
    "\n",
    "        n_channels, n_samples = wave.shape\n",
    "        channel_indices = np.arange(n_channels)\n",
    "        np.random.shuffle(channel_indices)\n",
    "        shuffled_wave = wave[channel_indices]\n",
    "        return shuffled_wave\n"
   ],
   "id": "464e51dd82cc1869",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### AverageMeter",
   "id": "8df07a7f76c1894f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.910873Z",
     "start_time": "2025-05-09T14:04:04.907014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class ROCAUCMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.y_true_11 = None\n",
    "        self.y_pred_11 = None\n",
    "\n",
    "    def update(self, y_true, y_pred):\n",
    "        y_true = y_true.cpu().numpy()\n",
    "\n",
    "        y_pred = torch.sigmoid(y_pred).data.cpu().numpy()\n",
    "\n",
    "        if self.y_true_11 is None:\n",
    "            self.y_true_11 = y_true\n",
    "            self.y_pred_11 = y_pred\n",
    "        else:\n",
    "            self.y_true_11 = np.concatenate((self.y_true_11, y_true), axis=0)\n",
    "            self.y_pred_11 = np.concatenate((self.y_pred_11, y_pred), axis=0)\n",
    "\n",
    "        return self.y_true_11, self.y_pred_11\n",
    "\n",
    "    def fast_auc(self, y_true, y_prob):\n",
    "\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_true = y_true[np.argsort(y_prob)]\n",
    "        cumfalses = np.cumsum(1 - y_true)\n",
    "        nfalse = cumfalses[-1]\n",
    "        auc = (y_true * cumfalses).sum()\n",
    "\n",
    "        auc /= (nfalse * (len(y_true) - nfalse))\n",
    "        return auc\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "\n",
    "        self.y_true_11 = self.y_true_11.reshape(-1)\n",
    "        self.y_pred_11 = self.y_pred_11.reshape(-1)\n",
    "        score = self.fast_auc(self.y_true_11, self.y_pred_11)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def evaluate(y_true, y_pred, digits=4, cutoff='auto'):\n",
    "\n",
    "        if cutoff == 'auto':\n",
    "            fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "            youden = tpr - fpr\n",
    "            cutoff = thresholds[np.argmax(youden)]\n",
    "\n",
    "        return cutoff\n",
    "\n",
    "    def report(self):\n",
    "\n",
    "        self.y_true_11 = self.y_true_11.reshape(-1)\n",
    "        self.y_pred_11 = self.y_pred_11.reshape(-1)\n",
    "\n",
    "        for score in range(1, 20):\n",
    "            score = score / 20\n",
    "            y_pre = self.y_pred_11 > score\n",
    "\n",
    "            tn, fp, fn, tp = confusion_matrix(self.y_true_11, y_pre).ravel()\n",
    "\n",
    "            precision = precision_score(self.y_true_11, y_pre)\n",
    "            recall = recall_score(self.y_true_11, y_pre)\n",
    "            f1 = f1_score(self.y_true_11, y_pre)\n",
    "\n",
    "            print('for threshold: %.4f, tn: %d,fp: %d,fn: %d,tp: %d,precision: %.4f, '\n",
    "                  'recall: %.4f, f1: %.4f' % (score, tn, fp, fn, tp, precision, recall, f1))\n",
    "\n",
    "        return score\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "5ae1a8b020308a6b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train",
   "id": "511219774d4b55e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:04.963247Z",
     "start_time": "2025-05-09T14:04:04.955512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class Train(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_df,\n",
    "                 val_df,\n",
    "                 fold):\n",
    "        self.train_df = train_df\n",
    "\n",
    "        self.train_generator = AlaskaDataIter(train_df, training_flag=True, shuffle=False)\n",
    "\n",
    "        self.train_ds = DataLoader(self.train_generator,\n",
    "                                   config.TRAIN.batch_size,\n",
    "                                   num_workers=config.TRAIN.process_num, shuffle=True)\n",
    "\n",
    "        self.val_generator = AlaskaDataIter(val_df, training_flag=False, shuffle=False)\n",
    "\n",
    "        self.val_ds = DataLoader(self.val_generator,\n",
    "                                 config.TRAIN.validatiojn_batch_size,\n",
    "                                 num_workers=config.TRAIN.process_num, shuffle=False)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.fold = fold\n",
    "\n",
    "        self.init_lr = config.TRAIN.init_lr\n",
    "        self.warmup_step = config.TRAIN.warmup_step\n",
    "        self.epochs = config.TRAIN.epoch\n",
    "        self.batch_size = config.TRAIN.batch_size\n",
    "        self.l2_regularization = config.TRAIN.weight_decay_factor\n",
    "        self.early_stop = config.MODEL.early_stop\n",
    "        self.accumulation_step = config.TRAIN.accumulation_batch_size // config.TRAIN.batch_size\n",
    "        self.gradient_clip = config.TRAIN.gradient_clip\n",
    "        self.is_base = config.is_base\n",
    "        self.save_dir = config.MODEL.model_path\n",
    "        self.fp16 = config.TRAIN.mix_precision\n",
    "\n",
    "        channel_num = 0\n",
    "        self.model = Net(add_channel=channel_num).to(self.device)\n",
    "        self.load_weight()\n",
    "\n",
    "        if 'Adamw' in config.TRAIN.opt:\n",
    "            self.optimizer = torch.optim.AdamW(self.model.parameters(),\n",
    "                                               lr=self.init_lr, eps=1.e-5,\n",
    "                                               weight_decay=self.l2_regularization)\n",
    "        else:\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(),\n",
    "                                             lr=self.init_lr,\n",
    "                                             momentum=0.9,\n",
    "                                             weight_decay=self.l2_regularization)\n",
    "\n",
    "        self.model = torch.nn.DataParallel(self.model)\n",
    "\n",
    "        self.iter_num = 0\n",
    "\n",
    "        if config.TRAIN.lr_scheduler == 'cos':\n",
    "            logger.info('lr_scheduler.CosineAnnealingLR')\n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer,\n",
    "                                                                        self.epochs,\n",
    "                                                                        eta_min=1.e-7)\n",
    "        else:\n",
    "            logger.info('lr_scheduler.ReduceLROnPlateau')\n",
    "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer,\n",
    "                                                                        mode='max',\n",
    "                                                                        patience=5,\n",
    "                                                                        min_lr=1e-7,\n",
    "                                                                        factor=config.TRAIN.lr_scheduler_factor,\n",
    "                                                                        verbose=True)\n",
    "\n",
    "        self.criterion = BinaryCrossEntropy(smoothing=0.1, pos_weight=torch.tensor(2.)).to(self.device)\n",
    "\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def custom_loop(self):\n",
    "\n",
    "        def distributed_train_epoch(epoch_num):\n",
    "\n",
    "            summary_loss = AverageMeter()\n",
    "            rocauc_score = ROCAUCMeter()\n",
    "            self.model.train()\n",
    "\n",
    "            for images, label in self.train_ds:\n",
    "\n",
    "                if epoch_num < 10:\n",
    "                    # excute warm up in the first epochs\n",
    "                    if self.warmup_step > 0:\n",
    "                        if self.iter_num < self.warmup_step:\n",
    "                            for param_group in self.optimizer.param_groups:\n",
    "                                param_group['lr'] = self.iter_num / float(self.warmup_step) * self.init_lr\n",
    "                                lr = param_group['lr']\n",
    "\n",
    "                            logger.info('warm up with learning rate: [%f]' % (lr))\n",
    "\n",
    "                start = time.time()\n",
    "\n",
    "                data = images.to(self.device).float()\n",
    "                label = label.to(self.device).float()\n",
    "\n",
    "                batch_size = data.shape[0]\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=self.fp16):\n",
    "                    predictions = self.model(data)\n",
    "                    current_loss = self.criterion(predictions, label)\n",
    "\n",
    "                summary_loss.update(current_loss.detach().item(), batch_size)\n",
    "                rocauc_score.update(label, predictions)\n",
    "                self.scaler.scale(current_loss).backward()\n",
    "\n",
    "                if ((self.iter_num + 1) % self.accumulation_step) == 0:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.gradient_clip, norm_type=2)\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                self.iter_num += 1\n",
    "                time_cost_per_batch = time.time() - start\n",
    "\n",
    "                images_per_sec = config.TRAIN.batch_size / time_cost_per_batch\n",
    "\n",
    "                if self.iter_num % config.TRAIN.log_interval == 0:\n",
    "                    log_message = '[fold %d], ' \\\n",
    "                                  'Train Step %d, ' \\\n",
    "                                  'summary_loss: %.6f, ' \\\n",
    "                                  'time: %.6f, ' \\\n",
    "                                  'speed %d images/persec' % (\n",
    "                                      self.fold,\n",
    "                                      self.iter_num,\n",
    "                                      summary_loss.avg,\n",
    "                                      time.time() - start,\n",
    "                                      images_per_sec)\n",
    "                    logger.info(log_message)\n",
    "\n",
    "            return summary_loss, rocauc_score\n",
    "\n",
    "        def distributed_test_epoch(epoch_num):\n",
    "\n",
    "            rocauc_score = ROCAUCMeter()\n",
    "            summary_loss = AverageMeter()\n",
    "            self.model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for (images, labels) in tqdm(self.val_ds):\n",
    "                    data = images.to(self.device).float()\n",
    "                    labels = labels.to(self.device).float()\n",
    "\n",
    "                    batch_size = data.shape[0]\n",
    "\n",
    "                    predictions = self.model(data)\n",
    "                    current_loss = self.criterion(predictions, labels)\n",
    "\n",
    "                    rocauc_score.update(labels, predictions)\n",
    "                    summary_loss.update(current_loss.detach().item(), batch_size)\n",
    "\n",
    "            return rocauc_score, summary_loss\n",
    "\n",
    "        best_distance = 0.\n",
    "        not_improvement = 0\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                lr = param_group['lr']\n",
    "            logger.info('learning rate: [%f]' % (lr))\n",
    "            t = time.time()\n",
    "\n",
    "            summary_loss, roc_auc_score = distributed_train_epoch(epoch)\n",
    "            train_epoch_log_message = '[fold %d], ' \\\n",
    "                                      '[RESULT]: TRAIN. Epoch: %d,' \\\n",
    "                                      ' summary_loss: %.5f,' \\\n",
    "                                      ' time:%.5f' % (\n",
    "                                          self.fold,\n",
    "                                          epoch,\n",
    "                                          summary_loss.avg,\n",
    "                                          (time.time() - t))\n",
    "            logger.info(train_epoch_log_message)\n",
    "            roc_auc_score.report()\n",
    "\n",
    "            if epoch % config.TRAIN.test_interval == 0:\n",
    "                roc_auc_score, summary_loss = distributed_test_epoch(epoch)\n",
    "\n",
    "                val_epoch_log_message = '[fold %d], ' \\\n",
    "                                        '[RESULT]: VAL. Epoch: %d,' \\\n",
    "                                        ' val_loss: %.5f,' \\\n",
    "                                        ' val_roc_auc: %.5f,' \\\n",
    "                                        ' time:%.5f' % (\n",
    "                                            self.fold,\n",
    "                                            epoch,\n",
    "                                            summary_loss.avg,\n",
    "                                            roc_auc_score.avg,\n",
    "                                            (time.time() - t))\n",
    "                logger.info(val_epoch_log_message)\n",
    "                roc_auc_score.report()\n",
    "\n",
    "            if config.TRAIN.lr_scheduler == 'cos':\n",
    "                self.scheduler.step()\n",
    "            else:\n",
    "                self.scheduler.step(roc_auc_score.avg)\n",
    "\n",
    "            # save model\n",
    "            if not os.access(config.MODEL.model_path, os.F_OK):\n",
    "                os.mkdir(config.MODEL.model_path)\n",
    "\n",
    "            #### save the model every end of epoch\n",
    "            current_model_saved_name = self.save_dir + '/fold%d_epoch_%d_val_rocauc_%.6f_loss_%.6f.pth' % (self.fold,\n",
    "                                                                                                           epoch,\n",
    "                                                                                                           roc_auc_score.avg,\n",
    "                                                                                                           summary_loss.avg)\n",
    "\n",
    "            logger.info('A model saved to %s' % current_model_saved_name)\n",
    "            torch.save(self.model.module.state_dict(), current_model_saved_name)\n",
    "\n",
    "            if summary_loss.avg < best_distance:\n",
    "                best_distance = summary_loss.avg\n",
    "                logger.info(' best loss value update as %.6f' % (best_distance))\n",
    "                logger.info(' bestmodel update as %s' % (current_model_saved_name))\n",
    "                not_improvement = 0\n",
    "\n",
    "            else:\n",
    "                not_improvement += 1\n",
    "\n",
    "            if not_improvement >= self.early_stop:\n",
    "                logger.info(' best metric score not improvement for %d, break' % (self.early_stop))\n",
    "                break\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def load_weight(self):\n",
    "        if config.MODEL.pretrained_model is not None:\n",
    "            state_dict = torch.load(config.MODEL.pretrained_model, map_location=self.device)\n",
    "            self.model.load_state_dict(state_dict, strict=False)\n"
   ],
   "id": "8f3f3fc75163b737",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Folding",
   "id": "3400db2d50ac9a6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:05.023463Z",
     "start_time": "2025-05-09T14:04:05.003426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_fold = 5\n",
    "\n",
    "\n",
    "def get_fold(n_fold=n_fold):\n",
    "    data = pd.read_csv(config.DATA.data_file)\n",
    "\n",
    "    folds = data.copy()\n",
    "    Fold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=config.SEED)\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['target'])):\n",
    "        folds.loc[val_index, 'fold'] = int(n)\n",
    "    return folds\n",
    "\n",
    "\n",
    "data = get_fold(n_fold)\n",
    "# print(data.columns)\n",
    "# data.head()\n",
    "\n",
    "\n"
   ],
   "id": "3082d89031b9bec",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "ac1d720d945540a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:58.407784Z",
     "start_time": "2025-05-09T14:04:05.050660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "# del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for fold in range(1):\n",
    "    ###build dataset\n",
    "\n",
    "    train_ind = data[data['train_val'] == 0].index.values\n",
    "    train_data = data.iloc[train_ind].copy()\n",
    "    val_ind = data[data['train_val'] == 1].index.values\n",
    "    val_data = data.iloc[val_ind].copy()\n",
    "    trainer = Train(train_df=train_data,\n",
    "                    val_df=val_data,\n",
    "                    fold=fold)\n",
    "    print(trainer.train_generator[100][0].shape)\n",
    "    # print(trainer.train_generator[100][0][0])\n",
    "    # print(trainer.val_generator[100][0].shape)\n",
    "    # print(trainer.val_generator[100][0][0])\n",
    "\n",
    "    # break\n",
    "    ### train\n",
    "    trainer.custom_loop()\n",
    "\n",
    "\n"
   ],
   "id": "2122aecc5c4b0d45",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-09 17:34:05,143] [INFO]  contains20360 samples  1979 pos \n",
      "[2025-05-09 17:34:05,143] [INFO]  contains20360 samples \n",
      "[2025-05-09 17:34:05,144] [INFO]  After filter contains20360 samples  1979 pos \n",
      "[2025-05-09 17:34:05,144] [INFO]  After filter contains20360 samples \n",
      "[2025-05-09 17:34:05,144] [INFO]  contains5089 samples  537 pos \n",
      "[2025-05-09 17:34:05,144] [INFO]  contains5089 samples \n",
      "[2025-05-09 17:34:05,145] [INFO]  After filter contains5089 samples  537 pos \n",
      "[2025-05-09 17:34:05,145] [INFO]  After filter contains5089 samples \n",
      "[2025-05-09 17:34:06,009] [INFO] Loading pretrained weights from Hugging Face hub (timm/vgg16.tv_in1k) \n",
      "[2025-05-09 17:34:06,562] [INFO] [timm/vgg16.tv_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors. \n",
      "[2025-05-09 17:34:06,563] [INFO] Converted input conv features.0 pretrained weights from 3 to 19 channel(s) \n",
      "[2025-05-09 17:34:06,790] [INFO] lr_scheduler.CosineAnnealingLR \n",
      "[2025-05-09 17:34:06,792] [INFO] learning rate: [0.000500] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-09 17:34:06,864] [INFO] warm up with learning rate: [0.000000] \n",
      "[2025-05-09 17:34:07,649] [INFO] warm up with learning rate: [0.000000] \n",
      "[2025-05-09 17:34:07,918] [INFO] warm up with learning rate: [0.000001] \n",
      "[2025-05-09 17:34:08,198] [INFO] warm up with learning rate: [0.000001] \n",
      "[2025-05-09 17:34:08,476] [INFO] warm up with learning rate: [0.000001] \n",
      "[2025-05-09 17:34:08,775] [INFO] warm up with learning rate: [0.000002] \n",
      "[2025-05-09 17:34:09,063] [INFO] warm up with learning rate: [0.000002] \n",
      "[2025-05-09 17:34:09,353] [INFO] warm up with learning rate: [0.000002] \n",
      "[2025-05-09 17:34:09,633] [INFO] warm up with learning rate: [0.000003] \n",
      "[2025-05-09 17:34:09,918] [INFO] warm up with learning rate: [0.000003] \n",
      "[2025-05-09 17:34:10,233] [INFO] [fold 0], Train Step 10, summary_loss: 0.866561, time: 0.314421, speed 407 images/persec \n",
      "[2025-05-09 17:34:10,235] [INFO] warm up with learning rate: [0.000003] \n",
      "[2025-05-09 17:34:10,559] [INFO] warm up with learning rate: [0.000004] \n",
      "[2025-05-09 17:34:10,842] [INFO] warm up with learning rate: [0.000004] \n",
      "[2025-05-09 17:34:11,120] [INFO] warm up with learning rate: [0.000004] \n",
      "[2025-05-09 17:34:11,455] [INFO] warm up with learning rate: [0.000005] \n",
      "[2025-05-09 17:34:11,779] [INFO] warm up with learning rate: [0.000005] \n",
      "[2025-05-09 17:34:12,063] [INFO] warm up with learning rate: [0.000005] \n",
      "[2025-05-09 17:34:12,345] [INFO] warm up with learning rate: [0.000006] \n",
      "[2025-05-09 17:34:12,664] [INFO] warm up with learning rate: [0.000006] \n",
      "[2025-05-09 17:34:12,946] [INFO] warm up with learning rate: [0.000006] \n",
      "[2025-05-09 17:34:13,270] [INFO] [fold 0], Train Step 20, summary_loss: 0.804262, time: 0.323843, speed 395 images/persec \n",
      "[2025-05-09 17:34:13,273] [INFO] warm up with learning rate: [0.000007] \n",
      "[2025-05-09 17:34:13,606] [INFO] warm up with learning rate: [0.000007] \n",
      "[2025-05-09 17:34:13,933] [INFO] warm up with learning rate: [0.000007] \n",
      "[2025-05-09 17:34:14,276] [INFO] warm up with learning rate: [0.000008] \n",
      "[2025-05-09 17:34:14,601] [INFO] warm up with learning rate: [0.000008] \n",
      "[2025-05-09 17:34:14,934] [INFO] warm up with learning rate: [0.000008] \n",
      "[2025-05-09 17:34:15,255] [INFO] warm up with learning rate: [0.000009] \n",
      "[2025-05-09 17:34:15,573] [INFO] warm up with learning rate: [0.000009] \n",
      "[2025-05-09 17:34:15,857] [INFO] warm up with learning rate: [0.000009] \n",
      "[2025-05-09 17:34:16,161] [INFO] warm up with learning rate: [0.000010] \n",
      "[2025-05-09 17:34:16,438] [INFO] [fold 0], Train Step 30, summary_loss: 0.747481, time: 0.276827, speed 462 images/persec \n",
      "[2025-05-09 17:34:16,440] [INFO] warm up with learning rate: [0.000010] \n",
      "[2025-05-09 17:34:16,720] [INFO] warm up with learning rate: [0.000010] \n",
      "[2025-05-09 17:34:17,000] [INFO] warm up with learning rate: [0.000011] \n",
      "[2025-05-09 17:34:17,279] [INFO] warm up with learning rate: [0.000011] \n",
      "[2025-05-09 17:34:17,556] [INFO] warm up with learning rate: [0.000011] \n",
      "[2025-05-09 17:34:17,836] [INFO] warm up with learning rate: [0.000012] \n",
      "[2025-05-09 17:34:18,116] [INFO] warm up with learning rate: [0.000012] \n",
      "[2025-05-09 17:34:18,395] [INFO] warm up with learning rate: [0.000012] \n",
      "[2025-05-09 17:34:18,675] [INFO] warm up with learning rate: [0.000013] \n",
      "[2025-05-09 17:34:18,953] [INFO] warm up with learning rate: [0.000013] \n",
      "[2025-05-09 17:34:19,229] [INFO] [fold 0], Train Step 40, summary_loss: 0.694682, time: 0.275402, speed 464 images/persec \n",
      "[2025-05-09 17:34:19,231] [INFO] warm up with learning rate: [0.000013] \n",
      "[2025-05-09 17:34:19,511] [INFO] warm up with learning rate: [0.000014] \n",
      "[2025-05-09 17:34:19,790] [INFO] warm up with learning rate: [0.000014] \n",
      "[2025-05-09 17:34:20,069] [INFO] warm up with learning rate: [0.000014] \n",
      "[2025-05-09 17:34:20,348] [INFO] warm up with learning rate: [0.000015] \n",
      "[2025-05-09 17:34:20,625] [INFO] warm up with learning rate: [0.000015] \n",
      "[2025-05-09 17:34:20,904] [INFO] warm up with learning rate: [0.000015] \n",
      "[2025-05-09 17:34:21,181] [INFO] warm up with learning rate: [0.000016] \n",
      "[2025-05-09 17:34:21,459] [INFO] warm up with learning rate: [0.000016] \n",
      "[2025-05-09 17:34:21,737] [INFO] warm up with learning rate: [0.000016] \n",
      "[2025-05-09 17:34:22,015] [INFO] [fold 0], Train Step 50, summary_loss: 0.659904, time: 0.277318, speed 461 images/persec \n",
      "[2025-05-09 17:34:22,017] [INFO] warm up with learning rate: [0.000017] \n",
      "[2025-05-09 17:34:22,293] [INFO] warm up with learning rate: [0.000017] \n",
      "[2025-05-09 17:34:22,571] [INFO] warm up with learning rate: [0.000017] \n",
      "[2025-05-09 17:34:22,848] [INFO] warm up with learning rate: [0.000018] \n",
      "[2025-05-09 17:34:23,129] [INFO] warm up with learning rate: [0.000018] \n",
      "[2025-05-09 17:34:23,409] [INFO] warm up with learning rate: [0.000018] \n",
      "[2025-05-09 17:34:23,689] [INFO] warm up with learning rate: [0.000019] \n",
      "[2025-05-09 17:34:23,966] [INFO] warm up with learning rate: [0.000019] \n",
      "[2025-05-09 17:34:24,244] [INFO] warm up with learning rate: [0.000019] \n",
      "[2025-05-09 17:34:24,522] [INFO] warm up with learning rate: [0.000020] \n",
      "[2025-05-09 17:34:24,798] [INFO] [fold 0], Train Step 60, summary_loss: 0.628002, time: 0.275577, speed 464 images/persec \n",
      "[2025-05-09 17:34:24,801] [INFO] warm up with learning rate: [0.000020] \n",
      "[2025-05-09 17:34:25,081] [INFO] warm up with learning rate: [0.000020] \n",
      "[2025-05-09 17:34:25,359] [INFO] warm up with learning rate: [0.000021] \n",
      "[2025-05-09 17:34:25,636] [INFO] warm up with learning rate: [0.000021] \n",
      "[2025-05-09 17:34:25,916] [INFO] warm up with learning rate: [0.000021] \n",
      "[2025-05-09 17:34:26,191] [INFO] warm up with learning rate: [0.000022] \n",
      "[2025-05-09 17:34:26,470] [INFO] warm up with learning rate: [0.000022] \n",
      "[2025-05-09 17:34:26,748] [INFO] warm up with learning rate: [0.000022] \n",
      "[2025-05-09 17:34:27,027] [INFO] warm up with learning rate: [0.000023] \n",
      "[2025-05-09 17:34:27,305] [INFO] warm up with learning rate: [0.000023] \n",
      "[2025-05-09 17:34:27,580] [INFO] [fold 0], Train Step 70, summary_loss: 0.598567, time: 0.274580, speed 466 images/persec \n",
      "[2025-05-09 17:34:27,583] [INFO] warm up with learning rate: [0.000023] \n",
      "[2025-05-09 17:34:27,863] [INFO] warm up with learning rate: [0.000024] \n",
      "[2025-05-09 17:34:28,146] [INFO] warm up with learning rate: [0.000024] \n",
      "[2025-05-09 17:34:28,424] [INFO] warm up with learning rate: [0.000024] \n",
      "[2025-05-09 17:34:28,702] [INFO] warm up with learning rate: [0.000025] \n",
      "[2025-05-09 17:34:28,980] [INFO] warm up with learning rate: [0.000025] \n",
      "[2025-05-09 17:34:29,257] [INFO] warm up with learning rate: [0.000025] \n",
      "[2025-05-09 17:34:29,537] [INFO] warm up with learning rate: [0.000026] \n",
      "[2025-05-09 17:34:29,814] [INFO] warm up with learning rate: [0.000026] \n",
      "[2025-05-09 17:34:30,094] [INFO] warm up with learning rate: [0.000026] \n",
      "[2025-05-09 17:34:30,369] [INFO] [fold 0], Train Step 80, summary_loss: 0.566858, time: 0.274712, speed 465 images/persec \n",
      "[2025-05-09 17:34:30,371] [INFO] warm up with learning rate: [0.000027] \n",
      "[2025-05-09 17:34:30,654] [INFO] warm up with learning rate: [0.000027] \n",
      "[2025-05-09 17:34:30,933] [INFO] warm up with learning rate: [0.000027] \n",
      "[2025-05-09 17:34:31,210] [INFO] warm up with learning rate: [0.000028] \n",
      "[2025-05-09 17:34:31,488] [INFO] warm up with learning rate: [0.000028] \n",
      "[2025-05-09 17:34:31,766] [INFO] warm up with learning rate: [0.000028] \n",
      "[2025-05-09 17:34:32,046] [INFO] warm up with learning rate: [0.000029] \n",
      "[2025-05-09 17:34:32,324] [INFO] warm up with learning rate: [0.000029] \n",
      "[2025-05-09 17:34:32,601] [INFO] warm up with learning rate: [0.000029] \n",
      "[2025-05-09 17:34:32,879] [INFO] warm up with learning rate: [0.000030] \n",
      "[2025-05-09 17:34:33,155] [INFO] [fold 0], Train Step 90, summary_loss: 0.546651, time: 0.276468, speed 462 images/persec \n",
      "[2025-05-09 17:34:33,158] [INFO] warm up with learning rate: [0.000030] \n",
      "[2025-05-09 17:34:33,437] [INFO] warm up with learning rate: [0.000030] \n",
      "[2025-05-09 17:34:33,715] [INFO] warm up with learning rate: [0.000031] \n",
      "[2025-05-09 17:34:33,993] [INFO] warm up with learning rate: [0.000031] \n",
      "[2025-05-09 17:34:34,272] [INFO] warm up with learning rate: [0.000031] \n",
      "[2025-05-09 17:34:34,550] [INFO] warm up with learning rate: [0.000032] \n",
      "[2025-05-09 17:34:34,829] [INFO] warm up with learning rate: [0.000032] \n",
      "[2025-05-09 17:34:35,109] [INFO] warm up with learning rate: [0.000032] \n",
      "[2025-05-09 17:34:35,386] [INFO] warm up with learning rate: [0.000033] \n",
      "[2025-05-09 17:34:35,664] [INFO] warm up with learning rate: [0.000033] \n",
      "[2025-05-09 17:34:35,941] [INFO] [fold 0], Train Step 100, summary_loss: 0.528123, time: 0.276763, speed 462 images/persec \n",
      "[2025-05-09 17:34:35,945] [INFO] warm up with learning rate: [0.000033] \n",
      "[2025-05-09 17:34:36,223] [INFO] warm up with learning rate: [0.000034] \n",
      "[2025-05-09 17:34:36,500] [INFO] warm up with learning rate: [0.000034] \n",
      "[2025-05-09 17:34:36,777] [INFO] warm up with learning rate: [0.000034] \n",
      "[2025-05-09 17:34:37,058] [INFO] warm up with learning rate: [0.000035] \n",
      "[2025-05-09 17:34:37,335] [INFO] warm up with learning rate: [0.000035] \n",
      "[2025-05-09 17:34:37,613] [INFO] warm up with learning rate: [0.000035] \n",
      "[2025-05-09 17:34:37,889] [INFO] warm up with learning rate: [0.000036] \n",
      "[2025-05-09 17:34:38,170] [INFO] warm up with learning rate: [0.000036] \n",
      "[2025-05-09 17:34:38,447] [INFO] warm up with learning rate: [0.000036] \n",
      "[2025-05-09 17:34:38,722] [INFO] [fold 0], Train Step 110, summary_loss: 0.509831, time: 0.274676, speed 466 images/persec \n",
      "[2025-05-09 17:34:38,725] [INFO] warm up with learning rate: [0.000037] \n",
      "[2025-05-09 17:34:39,007] [INFO] warm up with learning rate: [0.000037] \n",
      "[2025-05-09 17:34:39,284] [INFO] warm up with learning rate: [0.000037] \n",
      "[2025-05-09 17:34:39,567] [INFO] warm up with learning rate: [0.000038] \n",
      "[2025-05-09 17:34:39,845] [INFO] warm up with learning rate: [0.000038] \n",
      "[2025-05-09 17:34:40,125] [INFO] warm up with learning rate: [0.000038] \n",
      "[2025-05-09 17:34:40,403] [INFO] warm up with learning rate: [0.000039] \n",
      "[2025-05-09 17:34:40,682] [INFO] warm up with learning rate: [0.000039] \n",
      "[2025-05-09 17:34:40,960] [INFO] warm up with learning rate: [0.000039] \n",
      "[2025-05-09 17:34:41,237] [INFO] warm up with learning rate: [0.000040] \n",
      "[2025-05-09 17:34:41,513] [INFO] [fold 0], Train Step 120, summary_loss: 0.493258, time: 0.275585, speed 464 images/persec \n",
      "[2025-05-09 17:34:41,516] [INFO] warm up with learning rate: [0.000040] \n",
      "[2025-05-09 17:34:41,795] [INFO] warm up with learning rate: [0.000040] \n",
      "[2025-05-09 17:34:42,074] [INFO] warm up with learning rate: [0.000041] \n",
      "[2025-05-09 17:34:42,353] [INFO] warm up with learning rate: [0.000041] \n",
      "[2025-05-09 17:34:42,629] [INFO] warm up with learning rate: [0.000041] \n",
      "[2025-05-09 17:34:42,907] [INFO] warm up with learning rate: [0.000042] \n",
      "[2025-05-09 17:34:43,187] [INFO] warm up with learning rate: [0.000042] \n",
      "[2025-05-09 17:34:43,464] [INFO] warm up with learning rate: [0.000042] \n",
      "[2025-05-09 17:34:43,743] [INFO] warm up with learning rate: [0.000043] \n",
      "[2025-05-09 17:34:44,021] [INFO] warm up with learning rate: [0.000043] \n",
      "[2025-05-09 17:34:44,298] [INFO] [fold 0], Train Step 130, summary_loss: 0.479961, time: 0.276225, speed 463 images/persec \n",
      "[2025-05-09 17:34:44,300] [INFO] warm up with learning rate: [0.000043] \n",
      "[2025-05-09 17:34:44,582] [INFO] warm up with learning rate: [0.000044] \n",
      "[2025-05-09 17:34:44,858] [INFO] warm up with learning rate: [0.000044] \n",
      "[2025-05-09 17:34:45,136] [INFO] warm up with learning rate: [0.000044] \n",
      "[2025-05-09 17:34:45,414] [INFO] warm up with learning rate: [0.000045] \n",
      "[2025-05-09 17:34:45,694] [INFO] warm up with learning rate: [0.000045] \n",
      "[2025-05-09 17:34:45,974] [INFO] warm up with learning rate: [0.000045] \n",
      "[2025-05-09 17:34:46,273] [INFO] warm up with learning rate: [0.000046] \n",
      "[2025-05-09 17:34:46,552] [INFO] warm up with learning rate: [0.000046] \n",
      "[2025-05-09 17:34:46,830] [INFO] warm up with learning rate: [0.000046] \n",
      "[2025-05-09 17:34:47,107] [INFO] [fold 0], Train Step 140, summary_loss: 0.467181, time: 0.276883, speed 462 images/persec \n",
      "[2025-05-09 17:34:47,109] [INFO] warm up with learning rate: [0.000047] \n",
      "[2025-05-09 17:34:47,387] [INFO] warm up with learning rate: [0.000047] \n",
      "[2025-05-09 17:34:47,666] [INFO] warm up with learning rate: [0.000047] \n",
      "[2025-05-09 17:34:47,945] [INFO] warm up with learning rate: [0.000048] \n",
      "[2025-05-09 17:34:48,223] [INFO] warm up with learning rate: [0.000048] \n",
      "[2025-05-09 17:34:48,503] [INFO] warm up with learning rate: [0.000048] \n",
      "[2025-05-09 17:34:48,780] [INFO] warm up with learning rate: [0.000049] \n",
      "[2025-05-09 17:34:49,059] [INFO] warm up with learning rate: [0.000049] \n",
      "[2025-05-09 17:34:49,339] [INFO] warm up with learning rate: [0.000049] \n",
      "[2025-05-09 17:34:49,617] [INFO] warm up with learning rate: [0.000050] \n",
      "[2025-05-09 17:34:49,894] [INFO] [fold 0], Train Step 150, summary_loss: 0.458195, time: 0.276245, speed 463 images/persec \n",
      "[2025-05-09 17:34:49,897] [INFO] warm up with learning rate: [0.000050] \n",
      "[2025-05-09 17:34:50,178] [INFO] warm up with learning rate: [0.000050] \n",
      "[2025-05-09 17:34:50,457] [INFO] warm up with learning rate: [0.000051] \n",
      "[2025-05-09 17:34:50,735] [INFO] warm up with learning rate: [0.000051] \n",
      "[2025-05-09 17:34:51,014] [INFO] warm up with learning rate: [0.000051] \n",
      "[2025-05-09 17:34:51,292] [INFO] warm up with learning rate: [0.000052] \n",
      "[2025-05-09 17:34:51,572] [INFO] warm up with learning rate: [0.000052] \n",
      "[2025-05-09 17:34:51,852] [INFO] warm up with learning rate: [0.000052] \n",
      "[2025-05-09 17:34:52,130] [INFO] warm up with learning rate: [0.000053] \n",
      "[2025-05-09 17:34:52,407] [INFO] warm up with learning rate: [0.000053] \n",
      "[2025-05-09 17:34:52,618] [INFO] [fold 0], Train Step 160, summary_loss: 0.448199, time: 0.210260, speed 608 images/persec \n",
      "[2025-05-09 17:34:52,629] [INFO] [fold 0], [RESULT]: TRAIN. Epoch: 0, summary_loss: 0.44820, time:45.83675 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for threshold: 0.0500, tn: 4765,fp: 13616,fn: 84,tp: 1895,precision: 0.1222, recall: 0.9576, f1: 0.2167\n",
      "for threshold: 0.1000, tn: 7900,fp: 10481,fn: 213,tp: 1766,precision: 0.1442, recall: 0.8924, f1: 0.2483\n",
      "for threshold: 0.1500, tn: 10203,fp: 8178,fn: 369,tp: 1610,precision: 0.1645, recall: 0.8135, f1: 0.2736\n",
      "for threshold: 0.2000, tn: 11989,fp: 6392,fn: 561,tp: 1418,precision: 0.1816, recall: 0.7165, f1: 0.2897\n",
      "for threshold: 0.2500, tn: 13270,fp: 5111,fn: 707,tp: 1272,precision: 0.1993, recall: 0.6427, f1: 0.3042\n",
      "for threshold: 0.3000, tn: 14277,fp: 4104,fn: 826,tp: 1153,precision: 0.2193, recall: 0.5826, f1: 0.3187\n",
      "for threshold: 0.3500, tn: 15053,fp: 3328,fn: 959,tp: 1020,precision: 0.2346, recall: 0.5154, f1: 0.3224\n",
      "for threshold: 0.4000, tn: 15764,fp: 2617,fn: 1074,tp: 905,precision: 0.2570, recall: 0.4573, f1: 0.3290\n",
      "for threshold: 0.4500, tn: 16395,fp: 1986,fn: 1155,tp: 824,precision: 0.2932, recall: 0.4164, f1: 0.3441\n",
      "for threshold: 0.5000, tn: 16948,fp: 1433,fn: 1259,tp: 720,precision: 0.3344, recall: 0.3638, f1: 0.3485\n",
      "for threshold: 0.5500, tn: 17388,fp: 993,fn: 1352,tp: 627,precision: 0.3870, recall: 0.3168, f1: 0.3484\n",
      "for threshold: 0.6000, tn: 17721,fp: 660,fn: 1411,tp: 568,precision: 0.4625, recall: 0.2870, f1: 0.3542\n",
      "for threshold: 0.6500, tn: 17960,fp: 421,fn: 1461,tp: 518,precision: 0.5517, recall: 0.2617, f1: 0.3550\n",
      "for threshold: 0.7000, tn: 18095,fp: 286,fn: 1523,tp: 456,precision: 0.6146, recall: 0.2304, f1: 0.3352\n",
      "for threshold: 0.7500, tn: 18189,fp: 192,fn: 1565,tp: 414,precision: 0.6832, recall: 0.2092, f1: 0.3203\n",
      "for threshold: 0.8000, tn: 18239,fp: 142,fn: 1613,tp: 366,precision: 0.7205, recall: 0.1849, f1: 0.2943\n",
      "for threshold: 0.8500, tn: 18289,fp: 92,fn: 1646,tp: 333,precision: 0.7835, recall: 0.1683, f1: 0.2770\n",
      "for threshold: 0.9000, tn: 18323,fp: 58,fn: 1700,tp: 279,precision: 0.8279, recall: 0.1410, f1: 0.2409\n",
      "for threshold: 0.9500, tn: 18346,fp: 35,fn: 1755,tp: 224,precision: 0.8649, recall: 0.1132, f1: 0.2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  7.79it/s]\n",
      "[2025-05-09 17:34:57,894] [INFO] [fold 0], [RESULT]: VAL. Epoch: 0, val_loss: 0.44460, val_roc_auc: 0.90158, time:51.10180 \n",
      "[2025-05-09 17:34:57,953] [INFO] A model saved to ./trained_models//fold0_epoch_0_val_rocauc_0.901582_loss_0.444598.pth \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for threshold: 0.0500, tn: 3894,fp: 658,fn: 107,tp: 430,precision: 0.3952, recall: 0.8007, f1: 0.5292\n",
      "for threshold: 0.1000, tn: 4199,fp: 353,fn: 172,tp: 365,precision: 0.5084, recall: 0.6797, f1: 0.5817\n",
      "for threshold: 0.1500, tn: 4342,fp: 210,fn: 222,tp: 315,precision: 0.6000, recall: 0.5866, f1: 0.5932\n",
      "for threshold: 0.2000, tn: 4414,fp: 138,fn: 248,tp: 289,precision: 0.6768, recall: 0.5382, f1: 0.5996\n",
      "for threshold: 0.2500, tn: 4446,fp: 106,fn: 271,tp: 266,precision: 0.7151, recall: 0.4953, f1: 0.5853\n",
      "for threshold: 0.3000, tn: 4474,fp: 78,fn: 294,tp: 243,precision: 0.7570, recall: 0.4525, f1: 0.5664\n",
      "for threshold: 0.3500, tn: 4493,fp: 59,fn: 309,tp: 228,precision: 0.7944, recall: 0.4246, f1: 0.5534\n",
      "for threshold: 0.4000, tn: 4504,fp: 48,fn: 323,tp: 214,precision: 0.8168, recall: 0.3985, f1: 0.5357\n",
      "for threshold: 0.4500, tn: 4517,fp: 35,fn: 332,tp: 205,precision: 0.8542, recall: 0.3818, f1: 0.5277\n",
      "for threshold: 0.5000, tn: 4526,fp: 26,fn: 341,tp: 196,precision: 0.8829, recall: 0.3650, f1: 0.5165\n",
      "for threshold: 0.5500, tn: 4531,fp: 21,fn: 354,tp: 183,precision: 0.8971, recall: 0.3408, f1: 0.4939\n",
      "for threshold: 0.6000, tn: 4534,fp: 18,fn: 364,tp: 173,precision: 0.9058, recall: 0.3222, f1: 0.4753\n",
      "for threshold: 0.6500, tn: 4537,fp: 15,fn: 377,tp: 160,precision: 0.9143, recall: 0.2980, f1: 0.4494\n",
      "for threshold: 0.7000, tn: 4543,fp: 9,fn: 385,tp: 152,precision: 0.9441, recall: 0.2831, f1: 0.4355\n",
      "for threshold: 0.7500, tn: 4544,fp: 8,fn: 394,tp: 143,precision: 0.9470, recall: 0.2663, f1: 0.4157\n",
      "for threshold: 0.8000, tn: 4544,fp: 8,fn: 404,tp: 133,precision: 0.9433, recall: 0.2477, f1: 0.3923\n",
      "for threshold: 0.8500, tn: 4547,fp: 5,fn: 413,tp: 124,precision: 0.9612, recall: 0.2309, f1: 0.3724\n",
      "for threshold: 0.9000, tn: 4548,fp: 4,fn: 428,tp: 109,precision: 0.9646, recall: 0.2030, f1: 0.3354\n",
      "for threshold: 0.9500, tn: 4548,fp: 4,fn: 440,tp: 97,precision: 0.9604, recall: 0.1806, f1: 0.3041\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:58.418895Z",
     "start_time": "2025-05-09T14:04:58.417650Z"
    }
   },
   "cell_type": "code",
   "source": "# exit(0)",
   "id": "8a2acd4d97adaf76",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Validation",
   "id": "4f4b431f6d1a590d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:58.472819Z",
     "start_time": "2025-05-09T14:04:58.467850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_data_iter(test_path=config.DATA.data_file):\n",
    "    data = pd.read_csv(test_path)\n",
    "\n",
    "    val_ind = data[data['train_val'] == 1].index.values\n",
    "    val_data = data.iloc[val_ind].copy()\n",
    "\n",
    "    valds = AlaskaDataIter(val_data, training_flag=False, shuffle=False)\n",
    "    valds = DataLoader(valds,\n",
    "                       32,\n",
    "                       num_workers=2,\n",
    "                       shuffle=False)\n",
    "    return valds\n",
    "\n",
    "\n",
    "def get_model(weight, device, is_base=0):\n",
    "    channel_num = 0\n",
    "    if is_base == 0:\n",
    "        channel_num = 128\n",
    "    model = Net(add_channel=channel_num).to(device)\n",
    "    state_dict = torch.load(weight, map_location=device)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_add_plt(weight_video, weight_base, test_path):\n",
    "    rocauc_score = ROCAUCMeter()\n",
    "\n",
    "    base_y_true, base_y_pre = estimated_score(weight_base, test_path, 1)\n",
    "    video_y_true, video_y_pre = estimated_score(weight_video, test_path, 0)\n",
    "\n",
    "    print(\"========= estimated_score base line  ==========\", test_path)\n",
    "    rocauc_score.report_with_recall_precision(base_y_true, base_y_pre)\n",
    "    #rocauc_score.report_all(base_y_true, base_y_pre)\n",
    "\n",
    "    print(\"========= estimated_score add video ==========\", test_path)\n",
    "    rocauc_score.report_with_recall_precision(video_y_true, video_y_pre)\n",
    "    #rocauc_score.report_all(video_y_true, video_y_pre)\n",
    "\n",
    "    print(\"========= precision_recall ==========\", test_path)\n",
    "    img_path_p_r = test_path.split(\".\")[0] + \"_Precision_Recall__Add_Data_Pre\" + \".jpg\"\n",
    "    rocauc_score.report_with_recall(video_y_true, video_y_pre, base_y_true, base_y_pre, img_path_p_r)\n",
    "\n",
    "    print(\"========= Specificity_Sensitivity ==========\", test_path)\n",
    "    img_path_t_f = test_path.split(\".\")[0] + \"_Specificity_Sensitivity__Add_Data_Pre\" + \".jpg\"\n",
    "    rocauc_score.report_tpr_fpr(video_y_true, video_y_pre, base_y_true, base_y_pre, img_path_t_f)\n",
    "\n",
    "\n",
    "def estimated_score(weight, test_path, is_base):\n",
    "    # print(\"========= estimated_score test_path ==========\", test_path)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "    rocauc_score = ROCAUCMeter()\n",
    "    model = get_model(weight, device, is_base)\n",
    "    val_ds = get_data_iter(test_path)\n",
    "\n",
    "    labels_list = []\n",
    "    y_pre_list = []\n",
    "\n",
    "    y_true_11 = None\n",
    "    y_pred_11 = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"val_ds:\", val_ds)\n",
    "        for (images, labels, video_feature) in tqdm(val_ds):\n",
    "            data = images.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "            labels_list.append(labels)\n",
    "            # base_feature = base_feature.to(device).float()\n",
    "            video_feature = video_feature.to(device).float()\n",
    "            batch_size = data.shape[0]\n",
    "            predictions = model(data, video_feature, is_base)\n",
    "            y_pre_list.append(predictions)\n",
    "            y_true_11, y_pred_11 = rocauc_score.update(labels, predictions)\n",
    "            #print(\"=====y_true_11=====\", y_true_11)\n",
    "            #print(\"=====y_pred_11=====\", y_pred_11)\n",
    "    # save labels_list and y_pre_list\n",
    "    labels_data = torch.cat(labels_list, dim=0)\n",
    "    y_pre_data = torch.cat(y_pre_list, dim=0)\n",
    "    print(\"labels len:\", len(labels_data.tolist()))\n",
    "    print(\"predictions len:\", len(y_pre_data.tolist()))\n",
    "\n",
    "    return y_true_11, y_pred_11\n",
    "\n",
    "\n",
    "def eval(weight, test_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "    rocauc_score = ROCAUCMeter()\n",
    "    model = get_model(weight, device)\n",
    "    val_ds = get_data_iter(test_path)\n",
    "\n",
    "    labels_list = []\n",
    "    y_pre_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"val_ds:\", val_ds)\n",
    "        for (images, labels) in tqdm(val_ds):\n",
    "            data = images.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            labels_list.append(labels)\n",
    "            # batch_size = data.shape[0]\n",
    "            predictions = model(data)\n",
    "            # intermediate_output = model.intermediate_layer(data)\n",
    "            y_pre_list.append(predictions)\n",
    "            rocauc_score.update(labels, predictions)\n",
    "\n",
    "        labels_data = torch.cat(labels_list, dim=0)\n",
    "        y_pre_data = torch.cat(y_pre_list, dim=0)\n",
    "        rocauc_score.report()\n",
    "\n",
    "    print(\"labels len:\", len(labels_data.tolist()))\n",
    "    print(\"predictions len:\", len(y_pre_data.tolist()))\n",
    "\n",
    "    return rocauc_score\n",
    "\n",
    "\n"
   ],
   "id": "32eeedc553d6f049",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:05:20.956405Z",
     "start_time": "2025-05-09T14:05:16.779490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weight = os.path.join(config.MODEL.model_path, \"fold0_epoch_0_val_rocauc_0.901582_loss_0.444598.pth\")\n",
    "# test_path = \"/run/media/kami/SSD/DATASETS/vepiset-dataset/CSV-Files/data.csv\"\n",
    "test_path = config.DATA.data_file\n",
    "try:\n",
    "    eval(weight, test_path)\n",
    "except Exception as e:\n",
    "    print(\"=====e=====\", e)\n"
   ],
   "id": "a69be3fe5f91ea12",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-09 17:35:17,500] [INFO] Loading pretrained weights from Hugging Face hub (timm/vgg16.tv_in1k) \n",
      "[2025-05-09 17:35:17,758] [INFO] [timm/vgg16.tv_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors. \n",
      "[2025-05-09 17:35:17,760] [INFO] Converted input conv features.0 pretrained weights from 3 to 19 channel(s) \n",
      "[2025-05-09 17:35:18,093] [INFO]  contains5089 samples  537 pos \n",
      "[2025-05-09 17:35:18,094] [INFO]  contains5089 samples \n",
      "[2025-05-09 17:35:18,094] [INFO]  After filter contains5089 samples  537 pos \n",
      "[2025-05-09 17:35:18,094] [INFO]  After filter contains5089 samples \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_ds: <torch.utils.data.dataloader.DataLoader object at 0x765c536159d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 160/160 [00:02<00:00, 57.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for threshold: 0.0500, tn: 3893,fp: 659,fn: 107,tp: 430,precision: 0.3949, recall: 0.8007, f1: 0.5289\n",
      "for threshold: 0.1000, tn: 4199,fp: 353,fn: 172,tp: 365,precision: 0.5084, recall: 0.6797, f1: 0.5817\n",
      "for threshold: 0.1500, tn: 4341,fp: 211,fn: 222,tp: 315,precision: 0.5989, recall: 0.5866, f1: 0.5927\n",
      "for threshold: 0.2000, tn: 4414,fp: 138,fn: 248,tp: 289,precision: 0.6768, recall: 0.5382, f1: 0.5996\n",
      "for threshold: 0.2500, tn: 4446,fp: 106,fn: 271,tp: 266,precision: 0.7151, recall: 0.4953, f1: 0.5853\n",
      "for threshold: 0.3000, tn: 4474,fp: 78,fn: 294,tp: 243,precision: 0.7570, recall: 0.4525, f1: 0.5664\n",
      "for threshold: 0.3500, tn: 4493,fp: 59,fn: 309,tp: 228,precision: 0.7944, recall: 0.4246, f1: 0.5534\n",
      "for threshold: 0.4000, tn: 4504,fp: 48,fn: 323,tp: 214,precision: 0.8168, recall: 0.3985, f1: 0.5357\n",
      "for threshold: 0.4500, tn: 4517,fp: 35,fn: 332,tp: 205,precision: 0.8542, recall: 0.3818, f1: 0.5277\n",
      "for threshold: 0.5000, tn: 4526,fp: 26,fn: 341,tp: 196,precision: 0.8829, recall: 0.3650, f1: 0.5165\n",
      "for threshold: 0.5500, tn: 4531,fp: 21,fn: 354,tp: 183,precision: 0.8971, recall: 0.3408, f1: 0.4939\n",
      "for threshold: 0.6000, tn: 4534,fp: 18,fn: 364,tp: 173,precision: 0.9058, recall: 0.3222, f1: 0.4753\n",
      "for threshold: 0.6500, tn: 4537,fp: 15,fn: 377,tp: 160,precision: 0.9143, recall: 0.2980, f1: 0.4494\n",
      "for threshold: 0.7000, tn: 4543,fp: 9,fn: 385,tp: 152,precision: 0.9441, recall: 0.2831, f1: 0.4355\n",
      "for threshold: 0.7500, tn: 4544,fp: 8,fn: 394,tp: 143,precision: 0.9470, recall: 0.2663, f1: 0.4157\n",
      "for threshold: 0.8000, tn: 4544,fp: 8,fn: 404,tp: 133,precision: 0.9433, recall: 0.2477, f1: 0.3923\n",
      "for threshold: 0.8500, tn: 4547,fp: 5,fn: 413,tp: 124,precision: 0.9612, recall: 0.2309, f1: 0.3724\n",
      "for threshold: 0.9000, tn: 4548,fp: 4,fn: 428,tp: 109,precision: 0.9646, recall: 0.2030, f1: 0.3354\n",
      "for threshold: 0.9500, tn: 4548,fp: 4,fn: 440,tp: 97,precision: 0.9604, recall: 0.1806, f1: 0.3041\n",
      "labels len: 5089\n",
      "predictions len: 5089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:59.568340Z",
     "start_time": "2025-05-09T14:04:59.567106Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ba1710b6c17b9ea2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:04:59.616512Z",
     "start_time": "2025-05-09T14:04:59.615308Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f18a0d3b611b0819",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
